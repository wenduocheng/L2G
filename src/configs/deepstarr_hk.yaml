name: deepstarr_hk
environment:
    image:
        gpu: docker.io/junhongshen/nb360:updatenew
hyperparameters:
    dataset: deepstarr_hk
    embedder_dataset: text # hg38, text, text_roberta_large, text_llama, text_llama2, text_pythia_1b, text_flan_t5_small, text_flan_t5_base, text_flan_t5_large
    objective: MMD # otdd-exact, MMD
    weight: roberta # roberta, hyenadna-small-32k-seqlen, roberta-large, c, pythia-1b, llama, llama2, flan-t5-small, flan-t5-base, flan-t5-large
    nlayers: full # one, full
    maxsamples: 256 # 64, 256
    target_seq_len: 128 # 512 for roberta, 2048 for llama, 4096 for llama2, 2048 for pythia-1b, 1024 for flan t5

    experiment_id: 0

    seed: 0
    epochs: 20 # 13
    pretrain_epochs: 0 # 0, 20
    embedder_epochs: 30 # 60, 20
    predictor_epochs: 0

    joint_optim: True
    alpha: 1 # weight for otdd 
    beta: 1 # weight for bce
    finetune_method: all # all, lora, freeze
    one_hot: False # one hot encoding

    # lora_task_type: "SEQ_CLS"
    lora_r: 12 # 12
    lora_alpha: 32 # 32
    lora_dropout: 0.1
    lora_target_modules: ["q_proj", "v_proj"]

    drop_out: 0 #0
    label_smoothing_factor: 0
    activation: None
    rc_aug: True # reverse complement augmentation
    shift_aug: False # shift augmentation

    use_wandb: True
    wandb_key: "ef4b923327eb2a110fda334efee4ec80feee4bc7" # add your own wandb key
    data_parallel: False
    quantize: False

    embedder_type: "unet" # dash-->resnet, unet, dash random
    embedder_init: "random" # random, pretrained
    # ks: [9, 7, 5, 11, 5, 7, 9, 11, 11]  
    # ds: [1, 5, 3, 1, 3, 7, 1, 7, 7]
    # emedder_path: "/home/wenduoc/DASH/src_ablations/results_acc/human_enhancers_cohn/default/0/1/network_weights.pt"
    ks: [5, 3, 7, 11, 3, 9, 3, 5, 5, 7, 5, 11, 3, 7, 9, 5, 7, 5, 5, 3, 9, 9, 3, 7, 5, 9, 9, 5, 3, 7, 5, 5, 3, 3, 3, 5, 7, 5, 11, 11, 5, 5, 11, 7, 9, 3, 3, 5, 11, 9, 3, 3, 3, 11, 11, 7, 9, 3, 3, 9, 5, 3, 7, 3, 5, 7, 3, 3, 7, 3]  
    ds: [3, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3]
    # emedder_path: "/home/wenduoc/DASH/src/results_acc/human_enhancers_cohn/unet/unet/0/network_weights.pt"
    


    
    batch_size: 64
    eval_batch_size: 1000
    accum: 1
    clip: 1 
    validation_freq: 1

    optimizer:
        name: AdamW
        params:
            lr: 0.000005 # 0.00001 for pythia-1b, 0.001   
            betas: [0.9, 0.98] 
            weight_decay: 0.000001
            momentum: 0.99
    
    scheduler:  
        name: WarmupLR
        params:
            warmup_epochs: 5
            decay_epochs: 20
            sched: [20, 40, 60]
            base: 0.2

    no_warmup_scheduler:  
        name: StepLR
        params:
            warmup_epochs: 10
            decay_epochs: 100
            sched: [20, 40, 60]
            base: 0.2

    num_workers: 4
    reproducibility: False
    valid_split: False

min_validation_period:
    epochs: 1
bind_mounts:
    - host_path: /tmp
      container_path: /data
    - host_path: /tmp
      container_path: /root/.cache
resources:
  slots_per_trial: 1
records_per_epoch: 9281
searcher:
  name:  single
  metric: accuracy
  smaller_is_better: false
  max_length:
    epochs: 1
max_restarts: 0
entrypoint: python3 -W ignore main.py


