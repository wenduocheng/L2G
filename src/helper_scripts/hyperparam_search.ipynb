{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "from timeit import default_timer\n",
    "from attrdict import AttrDict\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "# from ml_collections import config_dict\n",
    "# from easydict import EasyDict as edict\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoConfig, RobertaForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import count_params, count_trainable_params, get_params_to_update, \\\n",
    "                            set_grad_state, set_param_grad\n",
    "from task_configs import get_data, get_config, get_metric, get_optimizer_scheduler, get_scheduler\n",
    "from embedder import get_tgt_model, wrapper1D\n",
    "import json\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"~/ORCA/clean/gene-orca\")\n",
    "    \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "def get_params_to_update(model):\n",
    "\n",
    "    params_to_update = []\n",
    "    name_list = ''\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            name_list += \"\\t\" + name\n",
    "    print(\"Params to learn:\", name_list)\n",
    "    return params_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(config):\n",
    "    if config['optimizer'] == 'SGD':\n",
    "        return partial(torch.optim.SGD, lr=config['lr'], momentum=0.99, weight_decay=config['weight_decay'])\n",
    "    elif config['optimizer'] == 'Adam':\n",
    "        return partial(torch.optim.Adam, lr=config['lr'], betas=[0.9, 0.98], weight_decay=config['weight_decay'])\n",
    "    elif config['optimizer'] == 'AdamW':\n",
    "        return partial(torch.optim.AdamW, lr=config['lr'], betas=[0.9, 0.98], weight_decay=config['weight_decay'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(args, model, optimizer, scheduler, loader, loss, temp, label_smoothing_factor=None, decoder=None, transform=None):    \n",
    "\n",
    "    model.train()\n",
    "                    \n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    right, alldata = 0,0\n",
    "    for i, data in enumerate(loader):\n",
    "\n",
    "        x, y = data \n",
    "            \n",
    "        x, y = x.to(args.device), y.to(args.device) # accelerate\n",
    "        out = model(x)\n",
    "\n",
    "        # right += (y==out.argmax(-1)).float().sum()  # if using accuracy, count how many out is correct (=y)\n",
    "        # alldata += len(x)\n",
    "\n",
    "      \n",
    "        # print('out:',out.size(),'y:', y.size())\n",
    "        l = loss(out, y)\n",
    "        \n",
    "        l.backward()\n",
    "\n",
    "\n",
    "        if config['clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['clip'])\n",
    "\n",
    "        if (i + 1) % config['accum'] == 0: # to save memory to approximate performance by large batchsize\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        \n",
    "        if args.lr_sched_iter:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss += l.item()\n",
    "\n",
    "        if i >= temp - 1:\n",
    "            break\n",
    "\n",
    "    if (not args.lr_sched_iter):\n",
    "        scheduler.step()\n",
    "    # print(right/alldata)\n",
    "    return train_loss / temp\n",
    "\n",
    "\n",
    "def evaluate(args, model, loader, loss, metric, n_eval, decoder=None, transform=None, fsd_epoch=None):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_score = 0, 0\n",
    "    right, alldata = 0,0\n",
    "\n",
    "\n",
    "    ys, outs, n_eval, n_data = [], [], 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader):\n",
    "            x, y = data\n",
    "                                \n",
    "            x, y = x.to(args.device), y.to(args.device) # accelerate\n",
    "\n",
    "            out = model(x)\n",
    "\n",
    "            # right+=(out.argmax(-1)==y).float().sum() #\n",
    "            # alldata += len(x) #\n",
    "    \n",
    "            outs.append(out) \n",
    "            ys.append(y) \n",
    "            n_data += x.shape[0]\n",
    "        \n",
    "            if n_data >= args.eval_batch_size or i == len(loader) - 1:\n",
    "                outs = torch.cat(outs, 0)\n",
    "                ys = torch.cat(ys, 0)\n",
    "\n",
    "                eval_loss += loss(outs, ys).item()\n",
    "                # print('309',outs.shape)\n",
    "                # print('309',ys.shape)\n",
    "                # print('309',outs)\n",
    "                # print('309',ys)\n",
    "                # print(metric(outs, ys))\n",
    "                eval_score += metric(outs, ys).item()\n",
    "                \n",
    "                n_eval += 1\n",
    "\n",
    "                ys, outs, n_data = [], [], 0\n",
    "\n",
    "        eval_loss /= n_eval\n",
    "        eval_score /= n_eval\n",
    "\n",
    "\n",
    "   \n",
    "    # eval_score = 1-(right/alldata).detach().cpu().numpy() # if using accuracy\n",
    "    return eval_loss, eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# args = np.load('/home/wenduoc/ORCA/clean/gene-orca/results/' + dataset + '/all_' + exp_id + '/0/hparams.npy', allow_pickle=True).item()\n",
    "# print(args)\n",
    "# args= AttrDict(args)\n",
    "# # args=AttrDict({'dataset': 'H4', 'embedder_dataset': 'text', 'objective': 'MMD', 'weight': 'roberta-large', 'maxsamples': 256, 'target_seq_len': 128, 'experiment_id': 8, 'seed': 0, 'epochs': 20, 'embedder_epochs': 60, 'pretrain_epochs': 0, 'predictor_epochs': 0, 'joint_optim': True, 'alpha': 1, 'beta': 1, 'finetune_method': 'all', 'one_hot': False, 'lora_r': 12, 'lora_alpha': 32, 'lora_dropout': 0.1, 'lora_target_modules': ['q_proj', 'v_proj'], 'drop_out': 0, 'label_smoothing_factor': 0, 'activation': None, 'rc_aug': True, 'shift_aug': True, 'use_wandb': True, 'wandb_key': 'ef4b923327eb2a110fda334efee4ec80feee4bc7', 'data_parallel': False, 'quantize': False, 'embedder_type': 'unet', 'embedder_init': 'random', 'batch_size': 64, 'eval_batch_size': 1000, 'accum': 1, 'clip': 1, 'validation_freq': 1, 'optimizer': {'name': 'AdamW', 'params': {'lr': 5e-06, 'betas': [0.9, 0.98], 'weight_decay': 1e-06, 'momentum': 0.99}}, 'scheduler': {'name': 'WarmupLR', 'params': {'warmup_epochs': 5, 'decay_epochs': 20, 'sched': [30, 60, 90], 'base': 0.2}}, 'no_warmup_scheduler': {'name': 'StepLR', 'params': {'warmup_epochs': 10, 'decay_epochs': 100, 'sched': [40, 60, 80], 'base': 0.2}}, 'num_workers': 2, 'reproducibility': False, 'valid_split': False, 'device': 'cuda', 'infer_label': False, 'lr_sched_iter': True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def main(config):\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0) \n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    eval('setattr(torch.backends.cudnn, \"deterministic\", True)')\n",
    "    eval('setattr(torch.backends.cudnn, \"benchmark\", False)')\n",
    "            \n",
    "    args = config_dict.ConfigDict()\n",
    "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    args.dataset = 'H4' \n",
    "    args.weight = 'roberta'\n",
    "    args.eval_batch_size = 1000\n",
    "    args.maxsamples=256\n",
    "    args.target_seq_len: 128\n",
    "    \n",
    "    # args.accum = 1\n",
    "    args.validation_freq = 1\n",
    "    \n",
    "    args.finetune_method = 'all'\n",
    "    \n",
    "    # args.lora = config_dict.ConfigDict()\n",
    "    # args.lora.target_modules = 'query value key dense reduction'\n",
    "    # args.lora.layer_indices = False\n",
    "    # args.lora.layers_pattern = False\n",
    "    # args.lora.bias = 'none'\n",
    "    # args.lora.rank = 8\n",
    "    # args.lora.alpha = 16\n",
    "    \n",
    "    args.num_workers = 4\n",
    "    args.valid_split = False\n",
    "    args.epochs = 30\n",
    "\n",
    "    args.scheduler = config_dict.ConfigDict()\n",
    "    args.scheduler.name = 'WarmupLR'\n",
    "    args.scheduler.params = config_dict.ConfigDict()\n",
    "    args.scheduler.params.warmup_epochs = 5\n",
    "    args.scheduler.params.decay_epochs = 60\n",
    "    args.scheduler.params.sched = [20, 40, 60]\n",
    "    args.scheduler.params.base = 0.2\n",
    "\n",
    "    args.no_warmup_scheduler = config_dict.ConfigDict()\n",
    "    args.scheduler.name = 'StepLR'\n",
    "    args.scheduler.params = config_dict.ConfigDict()\n",
    "    args.scheduler.params.warmup_epochs = 5\n",
    "    args.scheduler.params.decay_epochs = 30\n",
    "    args.scheduler.params.sched = [30, 60, 90]\n",
    "    args.scheduler.params.base = 0.2\n",
    "    \n",
    "\n",
    "    root = '/home/wenduoc/ORCA/clean/gene-orca/datasets'\n",
    "    \n",
    "    print('torch.cuda.is_available():',torch.cuda.is_available())\n",
    "    print('device:', args.device)\n",
    "\n",
    "\n",
    "    dims, sample_shape, num_classes, loss, args = get_config(root, args)\n",
    "    # print(dims, sample_shape, num_classes, loss)\n",
    "\n",
    "    args.embedder_epochs = 0\n",
    "\n",
    "    wrapper_func = wrapper1D \n",
    "    model = wrapper_func(sample_shape, num_classes, weight=args.weight, \n",
    "                            train_epoch=args.embedder_epochs, activation=args.activation, \n",
    "                            target_seq_len=args.target_seq_len, drop_out=config['drop_out'], lora=args.lora)\n",
    "    model.output_raw = False\n",
    "    model = model.to(args.device).train()\n",
    "    print(model)\n",
    "\n",
    "    # train_loader, val_loader, test_loader, n_train, n_val, n_test, data_kwargs = get_data(root, args.dataset, config['batch_size'], args.valid_split)\n",
    "    # decoder = data_kwargs['decoder'] if data_kwargs is not None and 'decoder' in data_kwargs else None \n",
    "    # transform = data_kwargs['transform'] if data_kwargs is not None and 'transform' in data_kwargs else None\n",
    "    # metric, compare_metrics = get_metric(root, args.dataset)\n",
    "\n",
    "\n",
    "    train_loader, val_loader, test_loader, n_train, n_val, n_test, data_kwargs = get_data(root, args.dataset, config['batch_size'], args.valid_split, quantize=False, rc_aug=True, shift_aug=True, one_hot=False)\n",
    "    metric, compare_metrics = get_metric(root, args.dataset)\n",
    "    decoder = None \n",
    "    transform = None\n",
    "    \n",
    "    train_full = True\n",
    "\n",
    "    # set whole model to be trainable\n",
    "    set_grad_state(model, True)  \n",
    "    set_param_grad(args, model, args.finetune_method)\n",
    "\n",
    "    optimizer = get_optimizer(config)(get_params_to_update(model))\n",
    "    lr_lambda, args.lr_sched_iter = get_scheduler(args.scheduler.name, args.scheduler.params, args.epochs, n_train)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "    # print(\"\\n------- Experiment Summary --------\")\n",
    "    # print(\"id:\", args.experiment_id)\n",
    "    # print(\"dataset:\", args.dataset, \"\\tbatch size:\", args.batch_size, \"\\tlr:\", args.optimizer.params.lr)\n",
    "    # print(\"num train batch:\", n_train, \"\\tnum validation batch:\", n_val, \"\\tnum test batch:\", n_test)\n",
    "    # print(\"finetune method:\", args.finetune_method)\n",
    "    # print('train_full:', train_full)\n",
    "    # print(\"param count:\", count_params(model), count_trainable_params(model))\n",
    "    \n",
    "    train_losses, train_score = [], []\n",
    "    for ep in range(args.epochs):\n",
    "\n",
    "        train_loss, model_time, data_time = train_one_epoch(args, config, model, optimizer, scheduler, train_loader, loss,  decoder, transform)\n",
    "        \n",
    "        if ep % args.validation_freq == 0 or ep == args.epochs-1: \n",
    "            val_loss, val_score = evaluate(args, model, val_loader, loss, metric, n_val, decoder, transform, \n",
    "                                           fsd_epoch=ep if args.dataset == 'FSD' else None)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            train_score.append(val_score)\n",
    "\n",
    "            train.report({'val_score': val_score})\n",
    "\n",
    "            print(\"[train full\", ep, \"]\",                    \n",
    "                    \"\\ttrain loss:\", \"%.4f\" % train_loss, \"\\tval loss:\", \"%.4f\" % val_loss, \n",
    "                    \"\\tval score:\", \"%.4f\" % val_score, \"\\tbest val score:\", \"%.4f\" % compare_metrics(train_score))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"lr\": tune.choice([5e-3, 5e-4, 5e-5, 5e-6]),\n",
    "    # \"weight_decay\": tune.choice([0, 1e-2, 1e-4]),\n",
    "    'batch_size': tune.choice([16, 32, 64]),\n",
    "    # 'accum': tune.choice([1,2]),\n",
    "    # 'clip': tune.choice([-1, 1]),\n",
    "    'drop_out': tune.choice([0, 0.05]),\n",
    "    'optimizer': tune.choice(['Adam', 'AdamW']),\n",
    "}\n",
    "\n",
    "# Uncomment this to enable distributed execution\n",
    "# `ray.init(address=\"auto\")`\n",
    "\n",
    "main_with_gpu = tune.with_resources(main, {\"cpu\": 8, \"gpu\": 1})\n",
    "tuner = tune.Tuner(\n",
    "    main_with_gpu,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=20,\n",
    "        scheduler=ASHAScheduler(metric=\"val_score\", mode=\"min\"),\n",
    "    ),\n",
    "    param_space=search_space,\n",
    ")\n",
    "results = tuner.fit()\n",
    "\n",
    "best_result = results.get_best_result(\"val_score\", \"min\")\n",
    "\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "\n",
    "# Obtain a trial dataframe from all run trials of this `tune.run` call.\n",
    "dfs = {result.path: result.metrics_dataframe for result in results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
